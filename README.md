# ğŸ”¥ Spark Insight Agent â€” Phase 2

**Production-grade AI assistant for Apache Spark incident analysis.**
Combines deterministic intelligence with LLM synthesis, multi-layer caching, and full cost tracking.

---

## Architecture

```
User Query
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. QUERY CACHE  (exact-match, 1h TTL)                           â”‚
â”‚     hit â†’ return instantly ($0 cost)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ miss
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. EMBEDDING LAYER  (text-embedding-3-small)                    â”‚
â”‚     + Embedding Cache (SHA-256 content hash, 24h TTL)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. QDRANT SEARCH  (top-10 by cosine similarity)                 â”‚
â”‚     Qdrant Cloud â€” collection: spark-incidents-openai            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. DETERMINISTIC ANALYZER                                       â”‚
â”‚     â€¢ Similarity scoring & spread (top-3 consistency)           â”‚
â”‚     â€¢ Root-cause cluster detection (12-category regex taxonomy) â”‚
â”‚     â€¢ Recurrence pattern flagging (â‰¥2 occurrences)             â”‚
â”‚     â€¢ Confidence score: weighted A+B+C+D (0.0 â€“ 1.0)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                         â”‚
   confidence â‰¥ 0.70         confidence < 0.70
          â”‚                         â”‚
          â–¼                         â–¼
   Structured Markdown        LLM Cache check
   (no LLM, ~$0 cost)              â”‚
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚ hit            â”‚ miss
                            â–¼                â–¼
                       cached answer    GPT-4o-mini call
                                        (30min LLM cache)
          â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. EVALUATOR  â€” per-query JSONL log                             â”‚
â”‚     tokens, cost, latency, confidence, path, cache hits         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
    â–¼
Streamlit UI (chat + metrics panel + sidebar stats)
```

---

## Module Reference

| Module | Purpose |
|--------|---------|
| `backend/config.py` | **Single source of truth** for all config values â€” models, thresholds, TTLs, cost rates. Load from `.env`. |
| `backend/cache_manager.py` | **3-layer cache**: query cache, embedding cache, LLM response cache. In-memory + pickle persistence. |
| `backend/deterministic_analyzer.py` | **Deterministic intelligence layer**: root-cause taxonomy, cluster detection, confidence scoring, answer generation without LLM. |
| `backend/evaluator.py` | **Metrics tracking**: cost, latency p95, path distribution, cache hit rates. Appends JSONL. |
| `backend/core_qdrant.py` | **Query pipeline**: wires all layers together. Phase 1 compatible return type. |
| `backend/data_pipeline.py` | **Ingestion**: PostgreSQL â†’ format â†’ embed â†’ Qdrant. Per-incident error handling. |
| `main.py` | **Streamlit UI**: chat + metrics panel per response + sidebar session stats. |

---

## Cost Comparison

| | Phase 1 | Phase 2 |
|--|---------|---------|
| **LLM model** | `gpt-4-turbo` ($10/$30 per 1M) | `gpt-4o-mini` ($0.15/$0.60 per 1M) |
| **LLM calls** | Every query | Only when confidence < 0.70 |
| **Caching** | None | 3 layers (query + embedding + LLM) |
| **Typical query cost** | ~$0.005 | ~$0.00002 (deterministic) / ~$0.0002 (LLM) |
| **Cost reduction** | â€” | **~25â€“250Ã—** depending on query mix |

---

## Confidence Score Components

```
A. Top similarity score        weight 0.40  â€” how close is the best match?
B. Score consistency (top-3)   weight 0.20  â€” are results tightly clustered?
C. Root-cause cohesion         weight 0.25  â€” do results agree on the cause?
D. Recurrence flag             weight 0.15  â€” is this a known repeat pattern?
```

When `A + B + C + D >= 0.70`, the deterministic layer returns a structured
answer with zero LLM cost. The LLM is only invoked for genuinely ambiguous
or novel queries.

---

## Quick Start

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Create .env
cat > .env <<EOF
OPENAI_API_KEY=sk-...
QDRANT_URL=https://your-cluster.qdrant.io
QDRANT_API_KEY=your-qdrant-key
DB_USER=postgres
DB_PASSWORD=your-db-password
EOF

# 3. Ingest incidents from PostgreSQL
python -m backend.data_pipeline

# 4. Launch Streamlit
streamlit run main.py
```

Open http://localhost:8501

---

## Example Queries

```
What caused the executor lost issue in INC-2024-001?
Show me all memory-related incidents
What clusters are experiencing data skew?
Summarize recurring OOM errors
What are the key learnings from shuffle failures?
```

---

## Metrics Log

Every query appends a JSON line to `metrics.jsonl`:

```json
{
  "timestamp": "2024-03-15T10:23:01.234Z",
  "confidence": 0.82,
  "path": "deterministic",
  "model_used": null,
  "embedding_tokens": 0,
  "llm_input_tokens": 0,
  "total_cost_usd": 0.0,
  "latency_ms": 312.4,
  "top_similarity": 0.913,
  "clusters_detected": 1,
  "query_cache_hit": false,
  "embedding_cache_hit": true
}
```

---

## Phase Roadmap

- **Phase 1** âœ… â€” Basic RAG: PostgreSQL â†’ Qdrant â†’ GPT-4-turbo â†’ Streamlit
- **Phase 2** âœ… â€” Deterministic intelligence + caching + cost tracking + evaluator
- **Phase 3** ğŸ”œ â€” Hybrid search (BM25 + dense), reranking, MCP Spark History Server integration
