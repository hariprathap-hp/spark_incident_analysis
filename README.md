# ğŸ”¥ Spark Incident Analysis Assistant
**AI-powered assistant for analyzing Apache Spark incidents using LangChain, OpenAI, and Qdrant**

---

## ğŸ¯ Purpose

This project demonstrates a **Retrieval-Augmented Generation (RAG)** system that helps engineers analyze **Spark incident reports** efficiently.  
Instead of reading through multiple logs or reports, users can query an assistant that retrieves the most relevant incidents and summarizes their root cause, resolutions, and key learnings.

---

## âš™ï¸ Implementation Overview

### ğŸ”¹ Architecture
User Query â”€â–º LangChain Retrieval Chain â”€â–º Qdrant (Vector DB)
â”‚
â–¼
OpenAI (Embeddings + GPT-4)
â”‚
â–¼
Contextual Root Cause Explanation


### ğŸ”¹ Components
| Component | Purpose |
|------------|----------|
| **LangChain** | Handles text chunking, retrieval chain, and prompt orchestration |
| **OpenAI** | Provides embeddings (`text-embedding-3-small`) and reasoning via `gpt-4-turbo` |
| **Qdrant** | Vector database storing Spark incident chunks |
| **Streamlit** | User interface for chat-based querying |

### ğŸ”¹ Dataset
The dataset consists of ~20 structured Spark incident reports (executor lost, shuffle failures, OOM errors, etc.) stored in `spark_incidents/`.

---

## ğŸ’° Current Cost Factor

- **Embeddings**: `text-embedding-3-small` â€” ~$0.02 per 1,000,000 tokens  
- **LLM Queries**: `gpt-4-turbo` â€” ~$0.01 per 1,000 input tokens, ~$0.03 per 1,000 output tokens  
- A typical query costs **less than $0.005**  
- HuggingFace embeddings can be used as a **free fallback** (by updating `USE_OPENAI=False`)

---

## ğŸš€ Future Improvements

### 1ï¸âƒ£ **MCP Integration**
- Integrate the [Kubeflow MCP for Spark History Server](https://github.com/kubeflow/mcp-apache-spark-history-server)
- Enables querying live Spark job metadata (tasks, stages, metrics)
- Converts this assistant into a **real-time Spark Reliability Copilot**

### 2ï¸âƒ£ **Hybrid Search**
- Combine **dense (embeddings)** + **sparse (keyword/BM25)** retrieval
- Improves recall for queries mixing technical terms + natural language

### 3ï¸âƒ£ **Re-ranking**
- Use **cross-encoder reranking** (e.g., `bge-reranker-large`) on top of Qdrant results
- Boosts precision by scoring top-10 retrieved chunks before sending to GPT

---

## ğŸ§© How to Run the App

1ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt

2ï¸âƒ£ Set OpenAI key
Create a .env file:
   OPENAI_API_KEY=sk-your-api-key

3ï¸âƒ£ Start Qdrant locally
docker run -p 6333:6333 qdrant/qdrant

4ï¸âƒ£ Ingest incidents into Qdrant
python ingestion_qdrant_openai.py

5ï¸âƒ£ Launch Streamlit app
streamlit run main.py

Then open http://localhost:8501

ğŸ’¬ Example Queries
What caused the executor lost issue in incident INC-2024-001?
Explain the resolution for shuffle fetch failures.
List incidents related to OutOfMemoryError in Spark drivers.
Summarize all incidents involving data skew.
What are the key learnings from memory-related failures?

ğŸ§  Summary

This project showcases an end-to-end, locally deployable RAG assistant for analyzing Spark incidents.
It combines:

OpenAI reasoning (semantic understanding),

Qdrant retrieval (efficient similarity search), and

LangChain orchestration (retrieval + synthesis).

With MCP, hybrid retrieval, and reranking, this can evolve into a production-grade Spark Reliability Copilot capable of reasoning over both historical incidents and live Spark job data.